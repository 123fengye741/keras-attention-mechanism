

AFTER LSTM
# ATTENTION PART STARTS HERE
# a = Permute((2, 1))(lstm_out)
# a = Reshape((lstm_units, TIME_STEPS))(a)
# a = Dense(TIME_STEPS, activation='softmax')(a)
# if SINGLE_ATTENTION_VECTOR:
#     a = Lambda(lambda x: K.mean(x, axis=1), name='attention_vec')(a)  # this is the attention vector!
#     a = RepeatVector(lstm_units)(a)
# else:
#     a = Lambda(lambda x: x, name='attention_vec')(a)  # trick to name a layer.
# a_probs = Permute((2, 1))(a)
# attention_mul = merge([lstm_out, a_probs], name='attention_mul', mode='mul')
# ATTENTION PART FINISHES HERE


BEFORE LSTM
# ATTENTION PART STARTS HERE
# a = Permute((2, 1))(inputs)
# a = Reshape((INPUT_DIM, TIME_STEPS))(a)
# a = Dense(TIME_STEPS, activation='softmax')(a)
# if SINGLE_ATTENTION_VECTOR:
#     a = Lambda(lambda x: K.mean(x, axis=1), name='attention_vec')(a)  # this is the attention vector!
#     a = RepeatVector(INPUT_DIM)(a)
# else:
#     a = Lambda(lambda x: x, name='attention_vec')(a)  # trick to name a layer.
# a_probs = Permute((2, 1))(a)
# attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')
# ATTENTION PART FINISHES HERE